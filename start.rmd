---
title: "data transformation"
author: "Max"
date: "2024-03-06"
output: html_document
---

df <- df %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))
## Library Load
```{r}
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(readr)
```

## Initial Dataframe

We are using the bigcities dataframe from {link}, this dataframe has a bunch of metrics for each city per each 
```{r}
#setwd("C:/Users/maxb1/OneDrive/Documents/UNC_Stuff/stor_565/final_project")
all_cities = read_csv("data/bigcitiesall.csv")
total = all_cities %>% filter(geo_label_citystate == "U.S. Total")
```

### Pivoting to create the dataframe
```{r, eval = FALSE}
imp = data %>% select(metric_item_label, geo_label_city, geo_fips_code, value,date_label)
wide = imp %>% pivot_wider( names_from = metric_item_label, values_from = value)
## Write csv
#write.csv(wide, "bigcities_wide.csv", row.names = FALSE)
```

## Dropping High Na or no variance Columns
```{r}
cities = read_csv("data/bigcities_wide.csv")
us_total = cities %>% filter(geo_label_city == "U.S. Total")
## Work with 2021 or 2020 data
new_cities = cities %>% filter(date_label == 2021)
na_count = colSums(is.na(new_cities)) / nrow(new_cities)
lowna = na_count[colSums(is.na(new_cities)) / nrow(new_cities) < .33]
lowna_cols = names(lowna)
cities_nona = new_cities %>% select(all_of(lowna_cols))
## Impute as dataframe
cities_clean <- cities_nona %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))
# Subset the dataframe to keep only columns with non-zero standard deviation
#cities_filtered <- cities_nona %>% select(names(non_zero_sd_columns))
```

Do we want to keep just 2020 and later data? for now, no
```{r, eval=FALSE}
data_2020 = data %>% filter(date_label != 2010)
na_count_2020 = colSums(is.na(data_2020)) / nrow(data_2020)
length(na_count_2020[colSums(is.na(data_2020)) / nrow(data_2020) < .33])
lowna = na_count[colSums(is.na(data)) / nrow(data) < .33]
lowna_cols = names(lowna)
```


## Response Variable: 
This section of our model will be attempting to predict the happiness of a city, from this source https://wallethub.com/edu/happiest-places-to-live/32619, using the data from our dataframe
```{r}
# Assuming the data is tab-separated
happy <- read_csv("data/happiest_places_to_live.csv")
happy <- happy %>% filter(!is.na(City))
head(happy)
## Edit names to make them equal
happy <- happy %>%
  mutate(City = ifelse(City == "Detroit, MI", "Detroit", City)) %>% 
  mutate(City = ifelse(City == "Washington, DC", "Washington", City)) %>% 
  mutate(City = ifelse(City == "New York", "New York City", City))
```

```{r}
## Join 
joined = cities_clean %>% left_join(happy, by = join_by(geo_label_city == City))
response_cols = joined %>% select(any_of(colnames(happy)))
colnames(response_cols)
```

Our goal is to see if we can predict happiness in the cities based on the variables we have. To begin, we will build some basic models. Our initial response variable will be Total Score.

The first step is going to be choosing suitable variables. We begin with 
his gives us 83 variables to work with across 36 cities and varying times

```{r}
#colnames(data1) # These are the variables
## label columns are geo_label_city, geo_fips_code
## Numerical columns
num_cols = cities_clean %>% select(!c(geo_label_city, geo_fips_code, date_label))%>%
  select(where(is.numeric))
par(mfrow=c(3,3), mar=c(4,4,2,0.5))
# Iterate over numeric columns and plot histograms
for (j in seq_along(num_cols)) {
  hist(num_cols[[j]], 
       xlab=names(num_cols)[j],
       main=paste("Histogram of", names(num_cols)[j]),
       col="lightblue", breaks=20)
}
```
There are a lot of variables, most seem to be fairly normal, although there does seem to be some categories that have similar distributions. Deaths, drug related variables, transporation variables, all seem to be pretty similar within its own group.

```{r}
imp_vars = joined %>% select(`Total Score`, any_of(colnames(num_cols)))
## 80 is too much to look at overall, but for some EDA going to look at first 10
library(corrr)
cor_matrix = cor(imp_vars, use = "na.or.complete")
cor_matrix[,1]
cor_melted <- as.data.frame(as.table(cor_matrix))

# Filter to remove self-correlations and get only one of each pair
cor_melted_filtered <- subset(cor_melted, Var1 != Var2 & as.numeric(Var1) < as.numeric(Var2))

# Filter for high correlations
high_cor_pairs <- subset(cor_melted_filtered, abs(Freq) > 0.75)
print(high_cor_pairs)
```
There are 400 pairs of very high correlation (>.75). Sources of this are:
  - Death Variables
  - Children related variables
  - Transportation Variables
  - Single Parent Families
  - Income Inequality related variables
  - Poverty related variables

Specific variables that have a lot of high correlation are:
  - Preterm Births
  - Premature Death
  - Life Expectancy
  - Single-Parent Families
  - Household Income Inequality
  - Service Workers
  

With this level of correlation, using this to classify something, even like total score will not yield much meaninful results. However, we will try to classify cities into categories/groupings, and attempt to explain the variation when it does vary from each other, and apply these groupings in other situations.

Things we could potentially predict:

- Consumer Price Index: https://fred.stlouisfed.org/series/CPIAUCSL
- Home Price Index: https://fred.stlouisfed.org/series/SPCS20RSA
- Meidcal Cost: https://fred.stlouisfed.org/series/CPIMEDSL
- Walkability: https://catalog.data.gov/dataset/walkability-index1
- Public School data: https://catalog.data.gov/dataset/public-school-characteristics-current-8d621
- College Scorecards: https://catalog.data.gov/dataset/college-scorecard-c25e9
- redlining: https://catalog.data.gov/dataset/redlining-maps-from-the-home-owners-loan-corporation-1937
- Zip code: https://catalog.data.gov/dataset/places-local-data-for-better-health-zcta-data-2020-release-ea5f2
- Places data: https://catalog.data.gov/dataset/places-local-data-for-better-health-county-data-2020-release-94305
