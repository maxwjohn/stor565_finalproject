---
title: "PCA"
author: "Nicolai"
date: "2024-03-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#Using Max's/Jebb's code for cleaning data

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
#setwd("/Users/nicolaibogh/Library/CloudStorage/OneDrive-KøbenhavnsUniversitet/Kandidat/UNC/STOR565 MACHINE LEARNING/StorGitHub/stor565MLproject")
data <- read_csv("bigcitiesall.csv")
total = data %>% filter(geo_label_citystate == "U.S. Total")
```



```{r}
#Clean data code (data defined as latest_filtered)
#Taking out na values and only using 2021
cities <- read_csv("bigcities_wide.csv")
us_total = cities %>% filter(geo_label_city == "U.S. Total")


#### From Max' cleaning
imp = data %>% select(metric_item_label, geo_label_city, geo_fips_code, value,date_label)
wide = imp %>% pivot_wider( names_from = metric_item_label, values_from = value)
## Write csv
# write.csv(wide, file = "/Users/nicolaibogh/Library/CloudStorage/OneDrive-KøbenhavnsUniversitet/Kandidat/UNC/STOR565 MACHINE LEARNING/StorGitHub/stor565MLproject/bigcities_wide.csv", row.names = FALSE)



cities = read_csv("bigcities_wide.csv")
us_total = cities %>% filter(geo_label_city == "U.S. Total")
## Work with 2021 or 2020 data
new_cities = cities %>% filter(date_label == 2021)
na_count = colSums(is.na(new_cities)) / nrow(new_cities)
lowna = na_count[colSums(is.na(new_cities)) / nrow(new_cities) < .33]
lowna_cols = names(lowna)
cities_nona = new_cities %>% select(all_of(lowna_cols))
## Impute as dataframe
cities_clean <- cities_nona %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))


numeric_data <- cities_clean[, -1]  # Exclude the first column containing city names
transposed_data <- numeric_data   # Transpose the numeric data
# Set column names to city names
# colnames(transposed_data) <- cities_clean$geo_label_city
# Now transposed_data contains the numeric variables as rows and cities as columns

# Removing "geo_fips_code" and "date_label"
transposed_data <- transposed_data[,-c(1:2) ]

```

```{r}
# replace transposed_data / city_var depending on if we want groupings for the variables or the cities
# Perform PCA
pca_result <- prcomp(transposed_data, scale. = TRUE)
summary(pca_result)
plot(pca_result$x[,1], pca_result$x[,2])
pca.var <- pca_result$sdev^2
# percantage of variation that each PCA accounts for
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```


```{r}
##### PCA continued
library(ggplot2)
 
pca.data <- data.frame(Sample=colnames(pca_result$x),
  X=pca_result$x[,1],
  Y=pca_result$x[,2])
pca.data
 
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("PCA Graph")
 
## get the name of the top 10 measurements (cities) that contribute most to pc1.
loading_scores <- pca_result$rotation[,1]
city_scores <- abs(loading_scores) ## get the magnitudes
city_score_ranked <- sort(city_scores, decreasing=TRUE)
top_10_cities <- names(city_score_ranked[1:10])
 
top_10_cities ## show the names of the top 10 cities
 
pca_result$rotation[top_10_cities,1]

```

# Hierarchical clustering with PCA
```{r}
num_components <- 3  # number of principal components

# Extracting PCA scores for selected components
pca_scores <- pca_result$x[, 1:num_components]

# Performing hierarchical clustering
hc_result <- hclust(dist(pca_scores), method = "complete")

# Visualizing hierarchical clustering results
plot(hc_result, main = "Hierarchical Clustering Dendrogram")

# heatmap to visualize clustered samples
cluster_order <- cutree(hc_result, k = 3)  
heatmap(pca_scores[order(cluster_order), ], scale = "row", 
        Rowv = NA, Colv = NA, col = heat.colors(256),
        main = "Clustered Samples Heatmap")

```

```{r}
# ELBOW METHOD
#set.seed(6)
# Make an empty vector we'll populate via our loop
wcss <- numeric(length = 10)

# For our 10 clusters we'll start with
for (i in 1:10) {
  wcss[i] <- sum(kmeans(pca_scores, i)$withinss)
}

plot(1:10,
     wcss,
     type = 'b', # for lines and points
     main = 'The Elbow Method',
     xlab = 'Number of clusters',
     ylab = 'WCSS')


```


```{r}
# Silhouette Analysis
library(cluster)

# Function to calculate silhouette score for hierarchical clustering
silhouette_score <- function(k, pca_scores){
  hc <- hclust(dist(pca_scores), method = "complete")
  cluster_assignments <- cutree(hc, k = k)
  ss <- silhouette(cluster_assignments, dist(pca_scores))
  mean(ss[, 3])
}

# Range of clusters to consider
k <- 2:10

# Calculate average silhouette scores for each number of clusters
avg_sil <- sapply(k, silhouette_score, pca_scores)

# Plot average silhouette scores
plot(k, avg_sil, type='b', xlab='Number of Clusters', ylab='Average Silhouette Scores', frame=FALSE,
     main = "Silhouette Analysis for Hierarchical Clustering")



library(factoextra)
# Performing hierarchical clustering
hc <- hclust(dist(pca_scores), method = "complete")

# Computing silhouette widths for different numbers of clusters
sil_width <- fviz_nbclust(pca_scores, FUNcluster = hcut, method = "silhouette")

# Plotting the results
print(sil_width)

cluster_assignments <- cutree(hc, k = 4)
# Visualizing the clusters
fviz_cluster(list(data = pca_scores, cluster = cluster_assignments),
             geom = "point", 
             stand = FALSE,  # preventing scaling of variables
             palette = "jco"
             )


```

## Cluster profiling

```{r}
# Extracting cluster assignments
# Retrieving the cluster assignments obtained from HC.
cluster_assignments <- cutree(hc_result, k = 3)  

# Profile clusters
library(dplyr)

# Combine cluster assignments with original data
clustered_data <- cbind(cluster_assignments, transposed_data)

# Calculating mean values of variables within each cluster
cluster_means <- clustered_data %>%
  group_by(cluster_assignments) %>%
  summarise_all(mean)
print(cluster_means)

# Identifying distinguishing variables
# Combine cluster assignments with original data
clustered_data <- cbind(cluster_assignments, transposed_data)

# Calculate mean or median values of variables within each cluster
cluster_summary <- clustered_data %>%
  group_by(cluster_assignments) %>%
  summarise_all(mean) 

# Identify distinguishing variables
# calculating coefficient of variation
coefficient_of_variation <- apply(cluster_summary[, -1], 2, function(x) sd(x) / mean(x))

# Printing variables with highest coefficient of variation
top_variables <- names(sort(coefficient_of_variation, decreasing = TRUE)[1:5])
print(top_variables)

# ANOVA, to perform statistical tests to assess significance
anova_results <- lapply(clustered_data[, -1], function(x) {
  aov_result <- aov(x ~ cluster_assignments, data = clustered_data)
  summary(aov_result)
})
# print(anova_results)

# visualizing the distribution of each variable across clusters using box plots, histograms, etc.
# Box plot for one variable
ggplot(clustered_data, aes(x = as.factor(cluster_assignments), y = Homicides)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "Homicides") +
  ggtitle("Distribution of Variable of Interest across Clusters")


# Get variables contributing most to the first principal component
loading_scores <- pca_result$rotation[,1]
top_variables <- names(sort(abs(loading_scores), decreasing = TRUE)[1:5])
print(top_variables)

# Compare mean values of these variables across clusters
top_variables_comparison <- clustered_data %>%
  group_by(cluster_assignments) %>%
  summarise(across(all_of(top_variables), mean))

# Visualize comparison
print(top_variables_comparison)


```


   - Premature Death: Cluster 3 has the highest mean premature death rate (16174), followed by cluster 2 (10398), and then cluster 1 (6699). This indicates that cluster 3 generally has a higher premature death rate compared to the other clusters.

  - Households with Higher Incomes: Cluster 1 has the highest mean percentage of households with higher incomes (63.9%), followed by cluster 2 (51.4%), and then cluster 3 (34.4%). This suggests that cluster 1 tends to have a higher proportion of households with higher incomes compared to the other clusters.

  - Life Expectancy: Cluster 1 has the highest mean life expectancy (78.5 years), followed by cluster 2 (73.6 years), and then cluster 3 (68.7 years). This indicates that cluster 1 generally has a higher life expectancy compared to the other clusters.


## Classification Task

In the classification task, I predict the cluster membership based on other variables.

```{r}





```







############## DO NOT USE #####################


TO DO:

PCA HC
  - Elbow method
  - High Dimentionality
  
1) Do initial Hierarchical Cluster(done)
2) Initial Kmeans Clustering(Jeb)
3) Do PCA → HC:(Nicolai)
    - PCA plot (kinda like Elbow)
4) Drop highly correlated variables: 10-20 variables (Max) --> HC
5) Compare clusters→get final clusters
6) Use this to predict on something




