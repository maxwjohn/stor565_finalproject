---
title: "PCA"
author: "Nicolai"
date: "2024-03-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#Using Max's/Jebb's code for cleaning data

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
#setwd("/Users/nicolaibogh/Library/CloudStorage/OneDrive-KøbenhavnsUniversitet/Kandidat/UNC/STOR565 MACHINE LEARNING/StorGitHub/stor565MLproject")
data <- read_csv("bigcitiesall.csv")
total = data %>% filter(geo_label_citystate == "U.S. Total")
```



```{r}
#Clean data code (data defined as latest_filtered)
#Taking out na values and only using 2021
cities <- read_csv("bigcities_wide.csv")
us_total = cities %>% filter(geo_label_city == "U.S. Total")


#### From Max' cleaning
imp = data %>% select(metric_item_label, geo_label_city, geo_fips_code, value,date_label)
wide = imp %>% pivot_wider( names_from = metric_item_label, values_from = value)
## Write csv
# write.csv(wide, file = "/Users/nicolaibogh/Library/CloudStorage/OneDrive-KøbenhavnsUniversitet/Kandidat/UNC/STOR565 MACHINE LEARNING/StorGitHub/stor565MLproject/bigcities_wide.csv", row.names = FALSE)



cities = read_csv("bigcities_wide.csv")
us_total = cities %>% filter(geo_label_city == "U.S. Total")
## Work with 2021 or 2020 data
new_cities = cities %>% filter(date_label == 2021)
na_count = colSums(is.na(new_cities)) / nrow(new_cities)
lowna = na_count[colSums(is.na(new_cities)) / nrow(new_cities) < .33]
lowna_cols = names(lowna)
cities_nona = new_cities %>% select(all_of(lowna_cols))
## Impute as dataframe
cities_clean <- cities_nona %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))


numeric_data <- cities_clean[, -1]  # Exclude the first column containing city names
transposed_data <- t(numeric_data)   # Transpose the numeric data
# Set column names to city names
colnames(transposed_data) <- cities_clean$geo_label_city
# Now transposed_data contains the numeric variables as rows and cities as columns

### test 2
city_var <- numeric_data
rownames(city_var) <- cities_clean$geo_label_city

# Removing "geo_fips_code" and "date_label"
transposed_data <- transposed_data[-c(1:2), ]


```

```{r}
# replace transposed_data / city_var depending on if we want groupings for the variables or the cities
# Perform PCA
pca_result <- prcomp(transposed_data, scale. = TRUE)
summary(pca_result)
plot(pca_result$x[,1], pca_result$x[,2])
pca.var <- pca_result$sdev^2
# percantage of variation that each PCA accounts for
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```

    - The PCA results show that the first principal component (PC1) explains the majority of the variation in the data, with a proportion of variance of 96.5%.
    - The subsequent principal components (PC2, PC3, etc.) each explain a smaller proportion of the variance, with PC2 explaining 2.4% and PC3 explaining 1.2% of the variance.
    - After PC3, the proportion of variance explained by each additional principal component becomes negligible (close to 0%).
  
The scree plot visualizes the proportion of variance explained by each principal component. In the scree plot, the first few principal components (PC1, PC2, and PC3) explained the majority of the variance, while subsequent components explain less and less. PC1, PC2, and PC3 are the most significant components, as they have relatively high proportions of variance.

```{r}
##### PCA continued
library(ggplot2)
 
pca.data <- data.frame(Sample=rownames(pca_result$x),
  X=pca_result$x[,1],
  Y=pca_result$x[,2])
pca.data
 
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("PCA Graph")
 
## get the name of the top 10 measurements (cities) that contribute most to pc1.
loading_scores <- pca_result$rotation[,1]
city_scores <- abs(loading_scores) ## get the magnitudes
city_score_ranked <- sort(city_scores, decreasing=TRUE)
top_10_cities <- names(city_score_ranked[1:10])
 
top_10_cities ## show the names of the top 10 cities
 
pca_result$rotation[top_10_cities,1]

```

# Hierarchical clustering with PCA
```{r}
num_components <- 3  # number of principal components

# Extracting PCA scores for selected components
pca_scores <- pca_result$x[, 1:num_components]

# Performing hierarchical clustering
hc_result <- hclust(dist(pca_scores), method = "complete")

# Visualizing hierarchical clustering results
plot(hc_result, main = "Hierarchical Clustering Dendrogram")

# heatmap to visualize clustered samples
cluster_order <- cutree(hc_result, k = 3)  
heatmap(pca_scores[order(cluster_order), ], scale = "row", 
        Rowv = NA, Colv = NA, col = heat.colors(256),
        main = "Clustered Samples Heatmap")

```

```{r}
# ELBOW METHOD
#set.seed(6)
# Make an empty vector we'll populate via our loop
wcss <- numeric(length = 10)

# For our 10 clusters we'll start with
for (i in 1:10) {
  wcss[i] <- sum(kmeans(pca_scores, i)$withinss)
}

plot(1:10,
     wcss,
     type = 'b', # for lines and points
     main = 'The Elbow Method',
     xlab = 'Number of clusters',
     ylab = 'WCSS')


```


```{r}
# Silhouette Analysis
library(cluster)

# Function to calculate silhouette score for hierarchical clustering
silhouette_score <- function(k, pca_scores){
  hc <- hclust(dist(pca_scores), method = "complete")
  cluster_assignments <- cutree(hc, k = k)
  ss <- silhouette(cluster_assignments, dist(pca_scores))
  mean(ss[, 3])
}

# Range of clusters to consider
k <- 2:10

# Calculate average silhouette scores for each number of clusters
avg_sil <- sapply(k, silhouette_score, pca_scores)

# Plot average silhouette scores
plot(k, avg_sil, type='b', xlab='Number of Clusters', ylab='Average Silhouette Scores', frame=FALSE,
     main = "Silhouette Analysis for Hierarchical Clustering")



library(factoextra)
# Perform hierarchical clustering
hc <- hclust(dist(pca_scores), method = "complete")

# Compute silhouette widths for different numbers of clusters
sil_width <- fviz_nbclust(pca_scores, FUNcluster = hcut, method = "silhouette")

# Plot the results
print(sil_width)



```













# hierarchical clustering OLD VERSION
```{r, fig.width=8, fig.height=10}
library(cluster)
library(dendextend)

# Only all numeric columns
data_for_clustering <- select(latest_filtered, where(is.numeric))

distance_matrix <- dist(data_for_clustering) # calculating distance matrix
hierarchical_clusters <- hclust(distance_matrix)

dend <- as.dendrogram(hierarchical_clusters) # Visualizing dendrogram with actual column names
labels(dend) <- colnames(data_for_clustering) # Adding names as labels
plot(dend, main = "Dendrogram for Hierarchical Clustering", horiz = TRUE)

########### Ordering clusters
# HC
num_clusters <- 5  # Adjustable
clusters <- cutree(hierarchical_clusters, k = num_clusters)

# order of observations
order <- order.dendrogram(dend)

# DF with the labels and cluster assignments
cluster_labels <- data.frame(Label = labels(dend)[order], Cluster = clusters[order])

View(cluster_labels)


```


I want to plot a graph of the within-cluster sum of squares (WSS) or variance against the number of clusters. The "elbow" point in the plot should represents the optimal number of clusters where adding more clusters does not significantly decrease the WSS. This method is more commonly used for k-means clustering but should also provide insights for hierarchical clustering.

To create a graph of the WSS or variance against the number of clusters (Elbow Method) for hierarchical clustering, we need to perform hierarchical clustering for different numbers of clusters and calculate the WSS or variance for each. However, hierarchical clustering does not directly provide WSS or variance values.

Instead, we can use the increase in the agglomeration coefficients between each step of the hierarchical clustering as a measure of "inertia" or "variance." The agglomeration coefficient represents the increase in distance between clusters as they are merged. A larger increase indicates that the clusters being merged are more different from each other, which can be interpreted as higher within-cluster variance.

    - I loop through different numbers of clusters from 2 to 10.
    - For each number of clusters, I perform hierarchical clustering using the hclust() function and calculate the WSS using the agglomeration coefficients.
    - I then plot the number of clusters against the WSS to visualize the Elbow Method graph.

```{r}
#################### Elbow Method  ####################
# Perform HC for different numbers of clusters
wss <- c()
for (i in 2:10) {
  hierarchical_clusters <- hclust(distance_matrix, method = "ward.D2")
  wss[i - 1] <- sum((hierarchical_clusters$height)^2)
}
# from ?hclust: the agglomeration method to be used. This should be (an unambiguous abbreviation of) one of "ward.D", "ward.D2", "single", "complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC).

# Plotting the Elbow Method graph
plot(2:10, wss, type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of Clusters", ylab = "Within-cluster Sum of Squares",
     main = "Elbow Method for Hierarchical Clustering")


```

Unfortunately, the plot for the Elbow Method graph appears as a horizontal line rather than having a clear "elbow" shape. This could suggest that there might not be a distinct optimal number of clusters based on the WSS or variance.

I have checked different hierarchical clustering methods (e.g., single linkage, complete linkage, average linkage, Ward's method) but they do not produce different results. The choice of method could affect the shape of the Elbow Method graph but doesn't in this case.

One reason why the plot for the Elbow Method graph appears as a horizontal line rather than having a clear "elbow" shape could be because of the dimensionality of the data. Since we have a high-dimensional dataset for the clustering (36 obs. of 94 variables), the Elbow Method may not be as effective, as the concept of "elbow" becomes less clear in higher-dimensional spaces.
Other factors could be the presence of noise or outliers in the data, which can also affect the clustering results and make it difficult to identify a clear elbow.

```{r}
# Calculating the hierarchical clustering
distance_matrix <- dist(data_for_clustering)
hierarchical_clusters <- hclust(distance_matrix)

# Cutting the dendrogram to get clusters
num_clusters <- 3  # Adjust this based on your dendrogram
clusters <- cutree(hierarchical_clusters, k = num_clusters)

# Calculating silhouette score
silhouette_score <- silhouette(clusters, dist = as.dist(distance_matrix))
print(silhouette_score)

# Plotting silhouette width
plot(silhouette_score, main = "Silhouette Plot for Hierarchical Clustering")
```

TO DO:

PCA HC
  - Elbow method
  - High Dimentionality
  
1) Do initial Hierarchical Cluster(done)
2) Initial Kmeans Clustering(Jeb)
3) Do PCA → HC:(Nicolai)
    - PCA plot (kinda like Elbow)
4) Drop highly correlated variables: 10-20 variables (Max) --> HC
5) Compare clusters→get final clusters
6) Use this to predict on something




