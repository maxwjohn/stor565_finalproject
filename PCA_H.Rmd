---
title: "PCA"
author: "Nicolai"
date: "2024-03-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#Using Max's/Jebb's code for cleaning data

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
#setwd("/Users/nicolaibogh/Library/CloudStorage/OneDrive-KøbenhavnsUniversitet/Kandidat/UNC/STOR565 MACHINE LEARNING/StorGitHub/stor565MLproject")
data <- read_csv("bigcitiesall.csv")
total = data %>% filter(geo_label_citystate == "U.S. Total")
```



```{r}
#Clean data code (data defined as latest_filtered)
#Taking out na values and only using 2021
cities <- read_csv("bigcities_wide.csv")
us_total = cities %>% filter(geo_label_city == "U.S. Total")


#### From Max' cleaning
imp = data %>% select(metric_item_label, geo_label_city, geo_fips_code, value,date_label)
wide = imp %>% pivot_wider( names_from = metric_item_label, values_from = value)
## Write csv
# write.csv(wide, file = "/Users/nicolaibogh/Library/CloudStorage/OneDrive-KøbenhavnsUniversitet/Kandidat/UNC/STOR565 MACHINE LEARNING/StorGitHub/stor565MLproject/bigcities_wide.csv", row.names = FALSE)



cities = read_csv("bigcities_wide.csv")
us_total = cities %>% filter(geo_label_city == "U.S. Total")
## Work with 2021 or 2020 data
new_cities = cities %>% filter(date_label == 2021)
na_count = colSums(is.na(new_cities)) / nrow(new_cities)
lowna = na_count[colSums(is.na(new_cities)) / nrow(new_cities) < .33]
lowna_cols = names(lowna)
cities_nona = new_cities %>% select(all_of(lowna_cols))
## Impute as dataframe
cities_clean <- cities_nona %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))


numeric_data <- cities_clean[, -1]  # Exclude the first column containing city names
transposed_data <- numeric_data   # Transpose the numeric data
# Set column names to city names
# colnames(transposed_data) <- cities_clean$geo_label_city
# Now transposed_data contains the numeric variables as rows and cities as columns

# Removing "geo_fips_code" and "date_label"
transposed_data <- transposed_data[,-c(1:2) ]
# removing US total )first row='
transposed_data <- transposed_data[-1, ]

```

```{r}
library(factoextra)

# replace transposed_data / city_var depending on if we want groupings for the variables or the cities
# Perform PCA
pca_result <- prcomp(transposed_data, scale. = TRUE)
summary(pca_result)
plot(pca_result$x[,1], pca_result$x[,2])
pca.var <- pca_result$sdev^2

# Check the standard deviations of the principal components
std_dev <- pca_result$sdev

# percantage of variation that each PCA accounts for
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")

# More focused version
fviz_eig(pca_result, addlabels = TRUE)
```

With the biplot, it is possible to visualize the similarities and dissimilarities between the samples, and further shows the impact of each attribute on each of the principal components.
```{r}
# Graph of the variables
fviz_pca_var(pca_result, col.var = "black")
```
Variables that are negatively correlated are displayed to the opposite sides of the biplot’s origin. 

The goal of the third visualization is to determine how much each variable is represented in a given component. Such a quality of representation is called the Cos2 and corresponds to the square cosine, and it is computed using the fviz_cos2 function.

  - A low value means that the variable is not perfectly represented by that component. 
  - A high value, on the other hand, means a good representation of the variable on that component.

```{r}
fviz_cos2(pca_result, choice = "var", axes = 1:2)
```

The code above computed the square cosine value for each variable with respect to the first two principal components. 
From the illustration above, X, X, X, and X are the top four variables with the highest cos2, hence contributing the most to PC1 and PC2.


***Biplot combined with cos2***

The last two visualization approaches: biplot and attributes importance can be combined to create a single biplot, where attributes with similar cos2 scores will have similar colors.  This is achieved by fine-tuning the fviz_pca_var function as follows:  

```{r}
fviz_pca_var(pca_result, col.var = "cos2",
             gradient.cols = c("black", "orange", "green"),
             repel = TRUE)
```
From the biplot above:

  - High cos2 attributes are colored in green:
  
  - Mid cos2 attributes have an orange color:
  
  - Finally, low cos2 attributes have a black color:

```{r}
##### PCA continued
library(ggplot2)
 
pca.data <- data.frame(Sample=colnames(pca_result$x),
  X=pca_result$x[,1],
  Y=pca_result$x[,2])
pca.data[1:12,]
 
ggplot(data=pca.data[1:12,], aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("PCA Graph")
 
## get the name of the top 10 measurements (cities) that contribute most to pc1.
loading_scores <- pca_result$rotation[,1]
city_scores <- abs(loading_scores) ## get the magnitudes
city_score_ranked <- sort(city_scores, decreasing=TRUE)
top_10_cities <- names(city_score_ranked[1:10])
 
top_10_cities ## show the names of the top 10 cities
 
pca_result$rotation[top_10_cities,1]

```

# Hierarchical clustering with PCA
```{r}
num_components <- 12  # number of principal components

# Extracting PCA scores for selected components
pca_scores <- pca_result$x[, 1:num_components]

# Performing hierarchical clustering
hc_result <- hclust(dist(pca_scores), method = "complete")

# Visualizing hierarchical clustering results
plot(hc_result, main = "Hierarchical Clustering Dendrogram")

# heatmap to visualize clustered samples
cluster_order <- cutree(hc_result, k = 3)  
heatmap(pca_scores[order(cluster_order), ], scale = "row", 
        Rowv = NA, Colv = NA, col = heat.colors(256),
        main = "Clustered Samples Heatmap")

```

```{r}
# ELBOW METHOD
#set.seed(6)
# Make an empty vector we'll populate via our loop
wcss <- numeric(length = 10)

# For our 10 clusters we'll start with
for (i in 1:10) {
  wcss[i] <- sum(kmeans(pca_scores, i)$withinss)
}

plot(1:10,
     wcss,
     type = 'b', # for lines and points
     main = 'The Elbow Method',
     xlab = 'Number of clusters',
     ylab = 'WCSS')


```


```{r}
# Silhouette Analysis
library(cluster)

# Function to calculate silhouette score for hierarchical clustering
silhouette_score <- function(k, pca_scores){
  hc <- hclust(dist(pca_scores), method = "complete")
  cluster_assignments <- cutree(hc, k = k)
  ss <- silhouette(cluster_assignments, dist(pca_scores))
  mean(ss[, 3])
}

# Range of clusters to consider
k <- 2:10

# Calculate average silhouette scores for each number of clusters
avg_sil <- sapply(k, silhouette_score, pca_scores)

# Plot average silhouette scores
plot(k, avg_sil, type='b', xlab='Number of Clusters', ylab='Average Silhouette Scores', frame=FALSE,
     main = "Silhouette Analysis for Hierarchical Clustering")



library(factoextra)
# Performing hierarchical clustering
hc <- hclust(dist(pca_scores), method = "complete")

# Computing silhouette widths for different numbers of clusters
sil_width <- fviz_nbclust(pca_scores, FUNcluster = hcut, method = "silhouette")

# Plotting the results
print(sil_width)

cluster_assignments <- cutree(hc, k = 4)
# Visualizing the clusters
fviz_cluster(list(data = pca_scores, cluster = cluster_assignments),
             geom = "point", 
             stand = FALSE,  # preventing scaling of variables
             palette = "jco"
             )


```

## Cluster profiling

```{r}
# Extracting cluster assignments
# Retrieving the cluster assignments obtained from HC.
cluster_assignments <- cutree(hc_result, k = 4)  

# Profile clusters
library(dplyr)

# Combine cluster assignments with original data
clustered_data <- cbind(cluster_assignments, transposed_data)

# Calculating mean values of variables within each cluster
cluster_means <- clustered_data %>%
  group_by(cluster_assignments) %>%
  summarise_all(mean)
print(cluster_means)

# Identifying distinguishing variables
# Combine cluster assignments with original data
clustered_data <- cbind(cluster_assignments, transposed_data)

# Calculate mean or median values of variables within each cluster
cluster_summary <- clustered_data %>%
  group_by(cluster_assignments) %>%
  summarise_all(mean) 

# Identify distinguishing variables
# calculating coefficient of variation
coefficient_of_variation <- apply(cluster_summary[, -1], 2, function(x) sd(x) / mean(x))

# Printing variables with highest coefficient of variation
top_variables <- names(sort(coefficient_of_variation, decreasing = TRUE)[1:5])
print(top_variables)

# ANOVA, to perform statistical tests to assess significance
anova_results <- lapply(clustered_data[, -1], function(x) {
  aov_result <- aov(x ~ cluster_assignments, data = clustered_data)
  summary(aov_result)
})
# print(anova_results)

# visualizing the distribution of each variable across clusters using box plots, histograms, etc.
# Box plot for one variable
ggplot(clustered_data, aes(x = as.factor(cluster_assignments), y = Homicides)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "Homicides") +
  ggtitle("Distribution of Variable of Interest across Clusters")


# Get variables contributing most to the first principal component
loading_scores <- pca_result$rotation[,1]
top_variables <- names(sort(abs(loading_scores), decreasing = TRUE)[1:5])
print(top_variables)

# Compare mean values of these variables across clusters
top_variables_comparison <- clustered_data %>%
  group_by(cluster_assignments) %>%
  summarise(across(all_of(top_variables), mean))

# Visualize comparison
print(top_variables_comparison)


```


   - Premature Death: Cluster 2 has the highest mean premature death rate (15215.), followed by cluster 1 (9834.), and then cluster 3 (6848.). This indicates that cluster 2 generally has a higher premature death rate compared to the other clusters.

  - Households with Higher Incomes: At the same time, Cluster 3 has the highest mean percentage of households with higher incomes (63.1%), followed by cluster 1 (52.6%), and then cluster 2 (39.2%). This suggests that cluster 3 tends to have a higher proportion of households with higher incomes compared to the other clusters.

  - Life Expectancy: Cluster 3 has the highest mean life expectancy (78.4 years), followed by cluster 1 (73.9 years), and then cluster 2 (69.6 years). This indicates that cluster 3 generally has a higher life expectancy compared to the other clusters.

```{r}
# Computing correlations between original variables and principal components
correlation_matrix <- cor(transposed_data, pca_result$x[, 1:12])

# Display correlation matrix
# print(correlation_matrix)

# Interpret correlations
# interpreting the first principal component, it will examine its correlations with the original variables.
correlations_pc1 <- correlation_matrix[, 1]  # Correlations with the first principal component
# print(correlations_pc1)
# Interpret other principal components in a similar manner

```





############## DO NOT USE #####################


TO DO:

PCA HC
  - Elbow method
  - High Dimentionality
  
1) Do initial Hierarchical Cluster(done)
2) Initial Kmeans Clustering(Jeb)
3) Do PCA → HC:(Nicolai)
    - PCA plot (kinda like Elbow)
4) Drop highly correlated variables: 10-20 variables (Max) --> HC
5) Compare clusters→get final clusters
6) Use this to predict on something

Use only the first 12 principals components


Sankey plot:



get the mean of each principal components
adult obesity
teen births


reduced dataset Max
newdf.csv

Use euclidian 

Ridge, Lasso, Elastic net?
Feature importance?

"ward performed best based on adjusted rand index"
